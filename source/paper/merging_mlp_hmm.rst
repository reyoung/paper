Merging MLP & HMM: Some Experiments in Continuous Speech Recognition
####################################################################

Introduction
------------

HMM被广泛的应用在语音识别领域，但是如果使用MLE(最大似然估计)训练的话，判别性(Discriminant Properties)弱。使用另一种作为评价准则的训练，最大互信息，会获得更大的判别性，但是有更多的约束和假设需要确定。最后，声学和语音的上下文信息，需要一个复杂的HMM，和更大的存储能力。

另一方面，多层感知器，作为连接性的架构，作为一个可选的工具用在了模式识别领域(例如语音识别)中。他最有用的性质是他的判别能力和学习能力，和表示不明确知识的能力。同时，上下文相关的信息可以很容易的被表示。但是MLP经常被用在非序列化的数据中。同时呢，加入延迟，反馈的神经网络可以提供动态的和不明确的记忆能力。很多人提议使用这个类型的架构。

在这篇文章讨论了将统计模型(HMM)和连接模型(MLP)联系起来。这个假说被发现，当使用HMM并使用MLP的潜在解。it is shown 在理论上和经验上来说，用MLP的输出输出来估算的类别间的概率分布来限制HMM的输入(也就是最大后验概率，在这里也被叫做贝叶斯概率)。同时，it is shown这些使用了上下文相关信息作为输入的MLP的估算，让Frame 分类的性能相对于没有上下文相关助益的MLE和MAP概率估算的HMM模型的性能明显提高。

同时需要明确的是，使用Frame和Phoneme来做识别的时候，用MLP使Frame和Phoneme的性能提升，并不会简单的导致连续语音识别的性能提升。很多的修改和改进才能使性能在词级别有可接受的提升。这个修改会被在后文中介绍。

HMM
---

在常见的离散HMM模型中，语音向量会在前端处理中被量化。每个向量被一个距离它最近的向量 :math:`y_i` 替换。这个向量是一个预先指定的有限集合 :math:`\mathfrak{Y}` ，其基为 :math:`I`. 让 :math:`\mathfrak{Q}` 作为一个有 :math:`K` 个不同状态的集合，每个状态为 :math:`q(k)` ，其中 :math:`k=1,...,K` 。马尔科夫模型是由这些状态，按照预定的拓扑结构组成的。如果HMM使用MLE评价标准来训练，那么模型的参数以最大化 :math:`P(X|W)` , 其中 :math:`X` 是训练使用的量化声学向量 :math:`x_n \in \mathfrak{Y}` ，其中 :math:`n=1,...,N` ,而 :math:`W` 与马尔科夫模型的 :math:`L` 个状态 :math:`q_l \in \mathfrak{Q}` 相关， 其中 :math:`l=1,..., L` 。当然， :math:`L \ne K \ne N` ，因为同样的状态可能在不同的位置出现多次， 也因为所有的状态不一定非要在一个模型里全部出现，也因为状态间的循环是被允许的。不考虑特定HMM模型的parenthesized index，我们将状态 :math:`q_l` 在指定时间 :math:`n \in [1,N]` 出现定义为 :math:`q_l^n` 。 由于 :math:`q_l^n` 是一个互斥实践，我们可以将概率 :math:`P(X|W)` 改写成，对于任何一个 :math:`n` :

.. math:: P(X|W)=\sum_{l=1}^L P(q_l^n, X|W),

这里， :math:`P(q_l^n, X|W)` 表示 :math:`X` 被参数 :math:`W` 限制的模型，同时 :math:`x_n` 与状态 :math:`q_l` 关联的概率。 最大化上式可以使用Baum-Welch算法中的前向后向算法循环计算出来。

